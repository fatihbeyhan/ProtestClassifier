{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,precision_score,recall_score,precision_recall_fscore_support,classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the extracted news as df\n",
    "data = pd.read_csv('extracted_data.csv').drop('Unnamed: 0', axis=1)\n",
    "data.columns = ['url','section','location','headline','text','label']\n",
    "\n",
    "\n",
    "# getting the extracted test news as df\n",
    "data_test = pd.read_csv('extracted_test_data.csv').drop('Unnamed: 0', axis=1)\n",
    "data_test.columns = ['url','section','location','headline','text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging headline and content as another column and create another dataframe with jusst text and label.\n",
    "df = pd.DataFrame([data.headline+\". \"+data.text,data.label]).transpose()\n",
    "df.columns = ['text','label']\n",
    "\n",
    "\n",
    "# merging headline and content as another column and create another dataframe with jusst text and label.\n",
    "df_test = pd.DataFrame([data_test.headline+\". \"+data_test.text,data_test.label]).transpose()\n",
    "df_test.columns = ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function \n",
    "    # split an article into sentences\n",
    "        # go to each sentence and split it to words\n",
    "            # if this word  is not in stopwords or other common words I've decided\n",
    "                #AND\n",
    "                   # if its alphabetic (getting rid of puctuation and numbers)\n",
    "                        #AND\n",
    "                            # if len of the word is greater than 2\n",
    "                            \n",
    "                            # lemmatize and lowercase the the word\n",
    "                            \n",
    "                            # return the cleaned article\n",
    "def preprocess(news):\n",
    "    l = WordNetLemmatizer()\n",
    "    sentences = news.split(\".\")\n",
    "    return \" \".join([l.lemmatize(word.lower()) for sentence in sentences for word in sentence.split() if word not in stopwords if word.isalpha() if len(word)> 2 if word.lower() not in [\"said\",\"the\",\"first\",\"also\",\"would\",\"one\",\"two\",\"they\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocess() function to each article\n",
    "df['text'] = df['text'].map(preprocess)\n",
    "\n",
    "\n",
    "\n",
    "# apply preprocess() function to each article\n",
    "df_test['text'] = df_test['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting protest and not_protest news\n",
    "protest_news = df[df.label == 1].text\n",
    "not_protest_news = df[df.label == 0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes theorem. \n",
    "    # finding the probability for being protest or not protest for an article which includes given word\n",
    "def word_affect(word):\n",
    "    number_of_occurance_in_protest_list = 0\n",
    "    for article in protest_news:\n",
    "        for w in article.split():\n",
    "            if w == word:\n",
    "                number_of_occurance_in_protest_list +=1\n",
    "                break \n",
    "                \n",
    "    number_of_occurance_in_not_protest_list = 0\n",
    "    for article in not_protest_news:\n",
    "        for w in article.split():\n",
    "            if w == word:\n",
    "                number_of_occurance_in_not_protest_list +=1\n",
    "                break       \n",
    "\n",
    "    \n",
    "    pi1 = len(protest_news)/(len(protest_news)+len(not_protest_news))\n",
    "    pi2 = len(not_protest_news)/(len(protest_news)+len(not_protest_news))\n",
    "    fkx = number_of_occurance_in_protest_list/len(protest_news)\n",
    "    fkx_ = number_of_occurance_in_not_protest_list/len(not_protest_news)\n",
    "    prob = (pi1*fkx)/((pi1*fkx)+(pi2*fkx_))\n",
    "    \n",
    "    \n",
    "    #returns (probability of protest, probability of not protest)\n",
    "        # this is might seem counterintutitive with word freqs but we must remember the ratio of news\n",
    "    return(prob,1-prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7532467532467533, 0.24675324675324672)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_affect(\"protest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df.text\n",
    "labels = df.label\n",
    "\n",
    "\n",
    "test_articles =  df_test.text\n",
    "test_labels = df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df= 10, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_articles = vectorizer.fit_transform(articles).toarray().astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581, 1227)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 943)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protest_target_words = []\n",
    "non_protest_target_words = []\n",
    "for word in list(vectorizer.vocabulary_.keys()):\n",
    "    if word_affect(word)[0]-word_affect(word)[1] > 0.2:\n",
    "        protest_target_words.append((word,(word_affect(word)[0],word_affect(word)[1])))\n",
    "        \n",
    "    elif word_affect(word)[1]-word_affect(word)[0] > 0.3 :\n",
    "        non_protest_target_words.append((word,(word_affect(word)[0],word_affect(word)[1])))\n",
    "        \n",
    "        \n",
    "len(protest_target_words),len(non_protest_target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gathering', (0.7272727272727273, 0.2727272727272727)),\n",
       " ('protest', (0.7532467532467533, 0.24675324675324672)),\n",
       " ('arrest', (0.6956521739130435, 0.30434782608695654)),\n",
       " ('deployed', (0.7999999999999999, 0.20000000000000007)),\n",
       " ('killed', (0.7272727272727273, 0.2727272727272727)),\n",
       " ('rally', (0.7999999999999999, 0.20000000000000007)),\n",
       " ('incident', (0.6785714285714286, 0.3214285714285714)),\n",
       " ('tdp', (0.6875, 0.3125)),\n",
       " ('demanding', (0.7659574468085106, 0.23404255319148937)),\n",
       " ('staged', (0.8333333333333334, 0.16666666666666663)),\n",
       " ('tried', (0.6470588235294118, 0.3529411764705882)),\n",
       " ('protester', (0.9333333333333332, 0.06666666666666676)),\n",
       " ('opposing', (0.7, 0.30000000000000004)),\n",
       " ('clash', (0.75, 0.25)),\n",
       " ('allegation', (0.7999999999999999, 0.20000000000000007)),\n",
       " ('injury', (0.7, 0.30000000000000004)),\n",
       " ('reached', (0.7142857142857143, 0.2857142857142857)),\n",
       " ('magistrate', (0.6153846153846154, 0.3846153846153846)),\n",
       " ('seeking', (0.611111111111111, 0.38888888888888895)),\n",
       " ('activist', (0.6923076923076923, 0.3076923076923077)),\n",
       " ('federation', (0.6666666666666666, 0.33333333333333337)),\n",
       " ('violence', (0.6499999999999999, 0.3500000000000001)),\n",
       " ('memorandum', (0.7142857142857143, 0.2857142857142857)),\n",
       " ('refused', (0.6666666666666666, 0.33333333333333337)),\n",
       " ('pending', (0.6363636363636364, 0.36363636363636365)),\n",
       " ('protesting', (0.7272727272727273, 0.2727272727272727)),\n",
       " ('slogan', (0.6666666666666666, 0.33333333333333337)),\n",
       " ('cadre', (0.7, 0.30000000000000004)),\n",
       " ('dharna', (0.9375, 0.0625)),\n",
       " ('attack', (0.7142857142857143, 0.2857142857142857)),\n",
       " ('blast', (0.7272727272727273, 0.2727272727272727)),\n",
       " ('petition', (0.6666666666666666, 0.33333333333333337)),\n",
       " ('hike', (0.75, 0.25)),\n",
       " ('agitation', (0.6666666666666666, 0.33333333333333337)),\n",
       " ('telangana', (0.7333333333333334, 0.2666666666666666)),\n",
       " ('wake', (0.6153846153846154, 0.3846153846153846)),\n",
       " ('injured', (0.8461538461538461, 0.15384615384615385)),\n",
       " ('attacked', (0.8333333333333334, 0.16666666666666663)),\n",
       " ('investigation', (0.6666666666666666, 0.33333333333333337)),\n",
       " ('demonstration', (0.9444444444444444, 0.05555555555555558)),\n",
       " ('policeman', (0.6666666666666666, 0.33333333333333337))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protest_target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for article in articles:\n",
    "    if len(set(article.split()))-len(set(article.split())-set([i[0] for i in protest_target_words]))>1:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       399\n",
      "           1       0.92      0.74      0.82       182\n",
      "\n",
      "    accuracy                           0.90       581\n",
      "   macro avg       0.91      0.86      0.88       581\n",
      "weighted avg       0.90      0.90      0.90       581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(results,list(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[388,  11],\n",
       "       [ 47, 135]], dtype=int64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(results,list(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for article in test_articles:\n",
    "    if len(set(article.split()))-len(set(article.split())-set([i[0] for i in protest_target_words]))>1:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       139\n",
      "           1       0.88      0.80      0.83        54\n",
      "\n",
      "    accuracy                           0.91       193\n",
      "   macro avg       0.90      0.88      0.89       193\n",
      "weighted avg       0.91      0.91      0.91       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(results,list(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[133,   6],\n",
       "       [ 11,  43]], dtype=int64)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(results,list(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = [i[0] for i in protest_target_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "for article in articles:\n",
    "    article_words = list(set(article.split()))\n",
    "    \n",
    "    article_vector = []\n",
    "    for word in WORDS:\n",
    "        if word in article_words:\n",
    "            article_vector.append(1)\n",
    "        else:\n",
    "            article_vector.append(0)\n",
    "    vectors.append(article_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vectors=[]\n",
    "for article in test_articles:\n",
    "    article_words = list(set(article.split()))\n",
    "    \n",
    "    article_vector = []\n",
    "    for word in WORDS:\n",
    "        if word in article_words:\n",
    "            article_vector.append(1)\n",
    "        else:\n",
    "            article_vector.append(0)\n",
    "    t_vectors.append(article_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93       148\n",
      "           1       0.76      0.82      0.79        45\n",
      "\n",
      "    accuracy                           0.90       193\n",
      "   macro avg       0.85      0.87      0.86       193\n",
      "weighted avg       0.90      0.90      0.90       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(SVC().fit(pd.DataFrame(vectors),list(labels)).predict(pd.DataFrame(t_vectors)),list(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8552187194886715"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = SVC()\n",
    "\n",
    "hyperparameters= dict(kernel=[\"linear\", \"poly\",\"sigmoid\"],\n",
    "                      C=np.logspace(0, 4, 5), \n",
    "                      class_weight=[{1:1,0:1},{1:2,0:1},{1:3,0:1},{1:3,0:0.5},{1:4,0:0.5},{1:4,0:1}])\n",
    "\n",
    "svc_grid = GridSearchCV(svc_model, hyperparameters, cv=5,scoring='f1_macro')\n",
    "\n",
    "best_svc = svc_grid.fit(pd.DataFrame(vectors),list(labels))\n",
    "\n",
    "best_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100.0, 'class_weight': {1: 2, 0: 1}, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       144\n",
      "           1       0.82      0.82      0.82        49\n",
      "\n",
      "    accuracy                           0.91       193\n",
      "   macro avg       0.88      0.88      0.88       193\n",
      "weighted avg       0.91      0.91      0.91       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(best_svc.predict(pd.DataFrame(t_vectors)),list(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513533597414323"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "\n",
    "\n",
    "hyperparameters = dict(C= np.logspace(0, 4, 10),\n",
    "                       class_weight = [{1:1,0:1},{1:2,0:1},{1:3,0:1},{1:3,0:0.5},{1:4,0:0.5},{1:4,0:1}])\n",
    "\n",
    "logistic_grid = GridSearchCV(logistic_model, hyperparameters, cv=5,scoring='f1_macro')\n",
    "\n",
    "best_logistic = logistic_grid.fit(pd.DataFrame(vectors),list(labels))\n",
    "\n",
    "best_logistic.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'class_weight': {1: 4, 0: 1}}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logistic.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       142\n",
      "           1       0.86      0.82      0.84        51\n",
      "\n",
      "    accuracy                           0.92       193\n",
      "   macro avg       0.90      0.89      0.89       193\n",
      "weighted avg       0.92      0.92      0.92       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(best_logistic.predict(pd.DataFrame(t_vectors)),list(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.tensor as tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(pd.DataFrame(vectors).to_numpy()).float()\n",
    "y_train = torch.from_numpy(np.array(list(labels))).float().view(len(labels),1)\n",
    "\n",
    "x_test = torch.from_numpy(pd.DataFrame(t_vectors).to_numpy()).float()\n",
    "y_test = torch.from_numpy(np.array(list(test_labels))).float().view(len(test_labels),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([581, 41]), torch.Size([581, 1]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x_train.dtype,y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "shuffled_idx = [i for i in range(len(y_train))]\n",
    "random.shuffle(shuffled_idx)\n",
    "\n",
    "batch_size = 581\n",
    "\n",
    "batches = []\n",
    "for i in range(0,len(y_train),batch_size):\n",
    "\n",
    "    indices= [shuffled_idx[i:i+batch_size]]\n",
    "\n",
    "    batches.append([x_train[indices],y_train[indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([581, 41]), torch.Size([581, 1]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0].shape,batches[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtestClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProtestClassifier,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(x_train.shape[1],64)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.layer2 = nn.Linear(64,1)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.layer3= nn.Linear(64,32)\n",
    "        self.drop3 = nn.Dropout(p=0.2)\n",
    "        self.layer4= nn.Linear(32,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def predict(self,x):\n",
    "        pred = self.forward(x)\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0]>0.500001:\n",
    "                ans.append(1)\n",
    "            else:\n",
    "                ans.append(0)\n",
    "        return torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "model = ProtestClassifier()\n",
    "#Define loss criterion\n",
    "criterion = nn.BCELoss()\n",
    "#Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t Training Loss: 0.214 Eval Loss: 0.2444\n",
      "20 \t Training Loss: 0.2051 Eval Loss: 0.2511\n",
      "30 \t Training Loss: 0.2124 Eval Loss: 0.2539\n",
      "40 \t Training Loss: 0.2057 Eval Loss: 0.2472\n",
      "50 \t Training Loss: 0.2166 Eval Loss: 0.2335\n",
      "60 \t Training Loss: 0.2049 Eval Loss: 0.2412\n",
      "\n",
      "\n",
      "Training Loss: 0.2139 Eval Loss: 0.2572\n"
     ]
    }
   ],
   "source": [
    "#Number of epochs\n",
    "epochs = 70\n",
    "#List to store losses\n",
    "trn_losses = []\n",
    "eval_losses = []\n",
    "for i in range(1,epochs):\n",
    "    for batch in batches:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        \n",
    "        y_pred = model.forward(x)\n",
    "        loss = criterion(y_pred,y)    \n",
    "\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        evalloss = criterion(model.forward(x_test),y_test)\n",
    "        print(i,'\\t',\"Training Loss:\",round(loss.item(),4),\"Eval Loss:\",round(evalloss.item(),4))\n",
    "        trn_losses.append(round(loss.item(),4))\n",
    "        eval_losses.append(round(evalloss.item(),4))\n",
    "\n",
    "evalloss = criterion(model.forward(x_test),y_test)\n",
    "print(\"\\n\\nTraining Loss:\",round(loss.item(),4),\"Eval Loss:\",round(evalloss.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       151\n",
      "           1       0.73      0.86      0.79        42\n",
      "\n",
      "    accuracy                           0.90       193\n",
      "   macro avg       0.85      0.89      0.86       193\n",
      "weighted avg       0.91      0.90      0.90       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(model.predict(x_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

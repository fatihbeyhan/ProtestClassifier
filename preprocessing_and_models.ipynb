{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,precision_recall_fscore_support,classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the extracted news as df\n",
    "data = pd.read_csv('extracted_data.csv').drop('Unnamed: 0', axis=1)\n",
    "data.columns = ['url','section','location','headline','text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging headline and content as another column and create another dataframe with jusst text and label.\n",
    "df = pd.DataFrame([data.headline+\". \"+data.text,data.label]).transpose()\n",
    "df.columns = ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Readers mail. SC ruling on alimony The Supreme...\n",
       "1      Niqaab is ‘the elephant in the courtroom’, say...\n",
       "2      Grandparents honoured. Staff and students of G...\n",
       "3      Fishermen call off strike. The country boat fi...\n",
       "4      Badal meets PM, seeks clemency for Bhullar. Pu...\n",
       "                             ...                        \n",
       "576    Profit not his motive. Taking cue from the sch...\n",
       "577    Music concert. A musical tribute to legendary ...\n",
       "578    Blind pursuit. The audience is tired of watchi...\n",
       "579    Pakistan kills 31 militants in airstrikes. Pak...\n",
       "580    Centre to hold talks to end Manipur crisis. Th...\n",
       "Name: text, Length: 581, dtype: object"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the text\n",
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function \n",
    "    # split an article into sentences\n",
    "        # go to each sentence and split it to words\n",
    "            # if this word  is not in stopwords or other common words I've decided\n",
    "                #AND\n",
    "                   # if its alphabetic (getting rid of puctuation and numbers)\n",
    "                        #AND\n",
    "                            # if len of the word is greater than 2\n",
    "                            \n",
    "                            # lemmatize and lowercase the the word\n",
    "                            \n",
    "                            # return the cleaned article\n",
    "def preprocess(news):\n",
    "    l = WordNetLemmatizer()\n",
    "    sentences = news.split(\".\")\n",
    "    return \" \".join([l.lemmatize(word.lower()) for sentence in sentences for word in sentence.split() if word not in stopwords if word.isalpha() if len(word)> 2 if word.lower() not in [\"said\",\"the\",\"first\",\"also\",\"would\",\"one\",\"two\",\"they\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocess() function to each article\n",
    "df['text'] = df['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      reader mail ruling alimony supreme court exten...\n",
       "1      niqaab elephant say judge person may give evid...\n",
       "2      grandparent honoured staff student global publ...\n",
       "3      fisherman call strike country boat fisherman l...\n",
       "4      badal meet seek clemency bhullar punjab chief ...\n",
       "                             ...                        \n",
       "576    profit motive taking cue scheme implemented st...\n",
       "577    music concert musical tribute legendary mystic...\n",
       "578    blind pursuit audience tired watching actor pl...\n",
       "579    pakistan kill militant airstrikes pakistani ai...\n",
       "580    centre hold talk end manipur crisis centre dec...\n",
       "Name: text, Length: 581, dtype: object"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Our data is ready to be fed into vectorizers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 word freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting protest and not_protest news\n",
    "protest_news = df[df.label == 1].text\n",
    "not_protest_news = df[df.label == 0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq function:\n",
    "    # this function will return the first_n frequent words with their ratio in a giving list of articles\n",
    "\n",
    "def give_freqs(list_of_articles,first_n=10):\n",
    "    freqs = {}\n",
    "    for n in list_of_articles:\n",
    "        words = set(n.split())\n",
    "\n",
    "        for w in words:\n",
    "            if w not in freqs:\n",
    "                freqs[w] = 1\n",
    "            else:\n",
    "                freqs[w] += 1\n",
    "\n",
    "    f = [(freqs[w]/len(list_of_articles),w) for w in freqs]\n",
    "    f.sort(reverse=True)\n",
    "\n",
    "    return f[:first_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4383561643835616, 'district'),\n",
       " (0.4315068493150685, 'government'),\n",
       " (0.4246575342465753, 'state'),\n",
       " (0.3972602739726027, 'protest'),\n",
       " (0.3698630136986301, 'police'),\n",
       " (0.2808219178082192, 'leader'),\n",
       " (0.2671232876712329, 'minister'),\n",
       " (0.2671232876712329, 'member'),\n",
       " (0.2465753424657534, 'demanding'),\n",
       " (0.23972602739726026, 'staged')]"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_freqs(protest_news,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2735632183908046, 'government'),\n",
       " (0.2482758620689655, 'year'),\n",
       " (0.23908045977011494, 'state'),\n",
       " (0.22528735632183908, 'take'),\n",
       " (0.21379310344827587, 'time'),\n",
       " (0.20689655172413793, 'people'),\n",
       " (0.20689655172413793, 'new'),\n",
       " (0.20689655172413793, 'minister'),\n",
       " (0.20229885057471264, 'day'),\n",
       " (0.19080459770114944, 'india')]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_freqs(not_protest_news,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These two list has some common words with different ratios lets get what are the differences in the top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'demanding', 'district', 'leader', 'member', 'police', 'protest', 'staged'}"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protest_words = set([i[1] for i in give_freqs(protest_news,10)])-set([i[1] for i in give_freqs(not_protest_news,10)])\n",
    "\n",
    "# these are protest words which are not in not_protest words\n",
    "protest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day', 'india', 'new', 'people', 'take', 'time', 'year'}"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_protest_words= set([i[1] for i in give_freqs(not_protest_news,10)])-set([i[1] for i in give_freqs(protest_news,10)])\n",
    "# same for the not_protest news\n",
    "not_protest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 word affects & distinctive words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, lets figure of what a single word can tell us about articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"protest\" affect:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as it can be seen on the word freqs \"protest\" is exists in the ~40% of the protest news and it does not exist on the not_protest news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes theorem. \n",
    "    # finding the probability for being protest or not protest for an article which includes given word\n",
    "def word_affect(word):\n",
    "    number_of_occurance_in_protest_list = 0\n",
    "    for article in protest_news:\n",
    "        for w in article.split():\n",
    "            if w == word:\n",
    "                number_of_occurance_in_protest_list +=1\n",
    "                break \n",
    "                \n",
    "    number_of_occurance_in_not_protest_list = 0\n",
    "    for article in not_protest_news:\n",
    "        for w in article.split():\n",
    "            if w == word:\n",
    "                number_of_occurance_in_not_protest_list +=1\n",
    "                break       \n",
    "\n",
    "    \n",
    "    pi1 = len(protest_news)/(len(protest_news)+len(not_protest_news))\n",
    "    pi2 = len(not_protest_news)/(len(protest_news)+len(not_protest_news))\n",
    "    fkx = number_of_occurance_in_protest_list/len(protest_news)\n",
    "    fkx_ = number_of_occurance_in_not_protest_list/len(not_protest_news)\n",
    "    prob = (pi1*fkx)/((pi1*fkx)+(pi2*fkx_))\n",
    "    \n",
    "    \n",
    "    #returns (probability of protest, probability of not protest)\n",
    "        # this is might seem counterintutitive with word freqs but we must remember the ratio of news\n",
    "    return(prob,1-prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7532467532467533, 0.24675324675324672)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_affect(\"protest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22222222222222224, 0.7777777777777778)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_affect(\"holiday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4864864864864864, 0.5135135135135136)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_affect(\"police\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 class disturbition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    435\n",
      "1    146\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x229c2d7be08>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.label.value_counts())\n",
    "df.label.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as it can be seen from the histogram, we have **unbalanced** data with ~25%,75% ratios.\n",
    "- this may cause some problems such as;\n",
    "    - focusing on wrong metrics, like accuracy, may mislead us. since, 75% of the data is 'class 0', if our model will label all the dataset as 'class 0', we'll get 75% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I will first try some algorithms on whole dataset and then try to handle the \"unbalanced data\" problem withh under-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training on Whole Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df.text\n",
    "labels = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text\n",
    "vectorizer = TfidfVectorizer(min_df= 4, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_articles = vectorizer.fit_transform(articles).toarray().astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dataframe = pd.DataFrame(tfidf_articles)\n",
    "tfidf_dataframe.reset_index().drop('index',axis=1)\n",
    "y = pd.DataFrame(labels).astype('float64').label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((581, 3390), (581,))"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dataframe.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx,testx,trainy,testy = train_test_split(tfidf_dataframe,y,test_size=0.3,shuffle=True,random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GridSearch for Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "\n",
    "\n",
    "hyperparameters = dict(C= np.logspace(0, 4, 10),\n",
    "                       penalty = ['l2'],\n",
    "                       class_weight = [{1:1,0:1},{1:2,0:1},{1:3,0:0.5}])\n",
    "\n",
    "logistic_grid = GridSearchCV(logistic_model, hyperparameters, cv=5,scoring='f1_macro')\n",
    "\n",
    "best_logistic = logistic_grid.fit(trainx, trainy)\n",
    "\n",
    "best_logistic.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 7.742636826811269, 'class_weight': {1: 3, 0: 0.5}, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logistic.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Scores For LogisticRegression()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.97      0.94       126\n",
      "         1.0       0.90      0.78      0.84        49\n",
      "\n",
      "    accuracy                           0.91       175\n",
      "   macro avg       0.91      0.87      0.89       175\n",
      "weighted avg       0.91      0.91      0.91       175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(best_logistic.predict(testx),testy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC()\n",
    "\n",
    "hyperparameters= dict(kernel=[\"linear\", \"poly\",\"sigmoid\"],\n",
    "                      C=np.logspace(0, 4, 10), \n",
    "                      class_weight=[{1:1,0:1},{1:2,0:1},{1:3,0:0.5}])\n",
    "\n",
    "svc_grid = GridSearchCV(svc_model, hyperparameters, cv=5,scoring='f1_macro')\n",
    "\n",
    "best_svc = svc_grid.fit(trainx,trainy)\n",
    "\n",
    "best_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'class_weight': {1: 3, 0: 0.5}, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Test Scores For SVC()___ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94       127\n",
      "         1.0       0.88      0.77      0.82        48\n",
      "\n",
      "    accuracy                           0.91       175\n",
      "   macro avg       0.90      0.87      0.88       175\n",
      "weighted avg       0.91      0.91      0.91       175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(best_svc.predict(testx),testy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this settings, Logistic Regression is doing slighlty better than SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, let's try same procedure for Under-Sampled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trainin on Under-Sampled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.concat([tfidf_dataframe,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_DATA = pd.concat([DATA[DATA.label == 0].sample(146),DATA[DATA.label==1]],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    146\n",
      "0.0    146\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x229b4a30ec8>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(undersampled_DATA.label.value_counts())\n",
    "undersampled_DATA.label.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### we achieved the balance by sacrificing on almost half of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "uns_x = undersampled_DATA.iloc[:,:-1]\n",
    "uns_y = undersampled_DATA.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx,testx,trainy,testy = train_test_split(uns_x,uns_y,test_size=0.3,shuffle=True,random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GridSearch for Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8564778001800128"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "\n",
    "\n",
    "hyperparameters = dict(C= np.logspace(0, 4, 10),\n",
    "                       penalty = ['l2'],\n",
    "                       class_weight = [{1:1,0:1},{1:2,0:1},{1:3,0:0.5}])\n",
    "\n",
    "logistic_grid = GridSearchCV(logistic_model, hyperparameters, cv=5,scoring='f1_macro')\n",
    "\n",
    "best_logistic = logistic_grid.fit(trainx, trainy)\n",
    "\n",
    "best_logistic.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 59.94842503189409, 'class_weight': {1: 1, 0: 1}, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logistic.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Scores For LogisticRegression()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.90      0.94        41\n",
      "         1.0       0.92      0.98      0.95        47\n",
      "\n",
      "    accuracy                           0.94        88\n",
      "   macro avg       0.95      0.94      0.94        88\n",
      "weighted avg       0.95      0.94      0.94        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(best_logistic.predict(testx),testy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8474970493391547"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = SVC()\n",
    "\n",
    "hyperparameters= dict(kernel=[\"linear\", \"poly\",\"sigmoid\"],\n",
    "                      C=np.logspace(0, 4, 10), \n",
    "                      class_weight=[{1:1,0:1},{1:2,0:1},{1:3,0:0.5}])\n",
    "\n",
    "svc_grid = GridSearchCV(svc_model, hyperparameters, cv=5,scoring='f1_macro')\n",
    "\n",
    "best_svc = svc_grid.fit(trainx,trainy)\n",
    "\n",
    "best_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'class_weight': {1: 2, 0: 1}, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Scores For SVC()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.90      0.92        40\n",
      "         1.0       0.92      0.96      0.94        48\n",
      "\n",
      "    accuracy                           0.93        88\n",
      "   macro avg       0.93      0.93      0.93        88\n",
      "weighted avg       0.93      0.93      0.93        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(best_svc.predict(testx),testy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With under-sampled data, I am getting better results and LogisticRegression is giving slightly better results than SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### working on this part but seems like will not work! dont worry about here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581, 3390) (581, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0735296125282273"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca_X = pd.DataFrame(pca.fit_transform(tfidf_dataframe))\n",
    "print (tfidf_dataframe.shape,pca_X.shape)\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.tensor as tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-431-fb77dcc1caf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m44\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "x_tr,x_te,y_tr,y_te = train_test_split(pca_X.to_numpy(),y.to_numpy(),test_size=0.2,random_state=44)\n",
    "\n",
    "x_train = torch.from_numpy(x_tr).float()\n",
    "y_train = torch.from_numpy(y_tr).float().view(len(y_tr),1)\n",
    "\n",
    "x_test = torch.from_numpy(x_te).float()\n",
    "y_test = torch.from_numpy(y_te).float().view(len(y_te),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([464, 30]), torch.Size([464, 1]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x.dtype,y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(t,e):\n",
    "    plt.plot(range(len(t)),t,color=\"r\")\n",
    "    plt.plot(range(len(t)),e,color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "shuffled_idx = [i for i in range(len(y_train))]\n",
    "random.shuffle(shuffled_idx)\n",
    "\n",
    "batch_size = 58\n",
    "\n",
    "batches = []\n",
    "for i in range(0,len(y_train),batch_size):\n",
    "\n",
    "    indices= [shuffled_idx[i:i+batch_size]]\n",
    "\n",
    "    batches.append([x_train[indices],y_train[indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([58, 30]), torch.Size([58, 1]))"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0].shape,batches[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtestClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProtestClassifier,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(x.shape[1],64)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.layer3= nn.Linear(32,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def predict(self,x):\n",
    "        pred = torch.sigmoid(self.forward(x))\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0]>0.500001:\n",
    "                ans.append(1)\n",
    "            else:\n",
    "                ans.append(0)\n",
    "        return torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "model = ProtestClassifier()\n",
    "#Define loss criterion\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 \t Training Loss: 0.6752 Eval Loss: 0.6529\n",
      "40 \t Training Loss: 0.6752 Eval Loss: 0.6528\n",
      "60 \t Training Loss: 0.6752 Eval Loss: 0.6527\n",
      "80 \t Training Loss: 0.6752 Eval Loss: 0.6528\n",
      "100 \t Training Loss: 0.6752 Eval Loss: 0.6527\n",
      "120 \t Training Loss: 0.6752 Eval Loss: 0.6528\n",
      "140 \t Training Loss: 0.6752 Eval Loss: 0.6527\n",
      "160 \t Training Loss: 0.6752 Eval Loss: 0.6528\n",
      "180 \t Training Loss: 0.6752 Eval Loss: 0.6527\n",
      "200 \t Training Loss: 0.6752 Eval Loss: 0.6528\n",
      "220 \t Training Loss: 0.6752 Eval Loss: 0.653\n",
      "240 \t Training Loss: 0.6752 Eval Loss: 0.6527\n",
      "260 \t Training Loss: 0.6752 Eval Loss: 0.6528\n",
      "280 \t Training Loss: 0.6752 Eval Loss: 0.6527\n",
      "300 \t Training Loss: 0.6752 Eval Loss: 0.6528\n",
      "320 \t Training Loss: 0.6752 Eval Loss: 0.6579\n",
      "340 \t Training Loss: 0.6752 Eval Loss: 0.658\n",
      "360 \t Training Loss: 0.6752 Eval Loss: 0.6579\n",
      "\n",
      "\n",
      "Training Loss: 0.6752 Eval Loss: 0.658\n"
     ]
    }
   ],
   "source": [
    "#Number of epochs\n",
    "epochs = 361\n",
    "#List to store losses\n",
    "trn_losses = []\n",
    "eval_losses = []\n",
    "for i in range(1,epochs):\n",
    "    for batch in batches:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        \n",
    "        y_pred = model.forward(x)\n",
    "        loss = criterion(y_pred,y)    \n",
    "\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        evalloss = criterion(model.forward(x_test),y_test)\n",
    "        print(i,'\\t',\"Training Loss:\",round(loss.item(),4),\"Eval Loss:\",round(evalloss.item(),4))\n",
    "        trn_losses.append(round(loss.item(),4))\n",
    "        eval_losses.append(round(evalloss.item(),4))\n",
    "\n",
    "evalloss = criterion(model.forward(x_test),y_test)\n",
    "print(\"\\n\\nTraining Loss:\",round(loss.item(),4),\"Eval Loss:\",round(evalloss.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
